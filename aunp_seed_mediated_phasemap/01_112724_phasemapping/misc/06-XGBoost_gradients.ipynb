{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pozzolabadmin/Documents/codebase/envs/activephasemap/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import xgboost\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch-compatible XGBoost model with training, prediction, saving, loading, \n",
    "    and backward pass support.\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "        self.evals_result = {}\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Train the XGBoost model with cross-validation and hyperparameter tuning.\n",
    "        \"\"\"\n",
    "        inputs_np = inputs.clone().detach().cpu().squeeze().numpy()\n",
    "        targets_np = targets.clone().detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        dtrain = xgboost.DMatrix(inputs_np, label=targets_np)\n",
    "\n",
    "        # Use cross-validation to evaluate the best parameters if param_grid is provided\n",
    "        param_grid = {\"eta\":[0.002, 0.4, 0.8], \"max_depth\": [2, 4, 6, 10]}\n",
    "        if param_grid:\n",
    "            best_params = None\n",
    "            best_score = float(\"inf\")\n",
    "            for params in ParameterGrid(param_grid):\n",
    "                # Merge given params with default ones\n",
    "                params = {**self.params, **params}\n",
    "                cv_results = xgboost.cv(\n",
    "                    params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=100,\n",
    "                    nfold=5,\n",
    "                    metrics=[\"rmse\"],\n",
    "                    early_stopping_rounds=10,\n",
    "                    seed=42\n",
    "                )\n",
    "                mean_rmse = cv_results[\"test-rmse-mean\"].min()\n",
    "                if mean_rmse < best_score:\n",
    "                    best_score = mean_rmse\n",
    "                    best_params = params\n",
    "            print(f\"Best Parameters: {best_params}, Best RMSE: {best_score}\")\n",
    "            self.params = best_params  # Update the model's parameters with the best ones\n",
    "\n",
    "        # Train the model using the best parameters\n",
    "        num_samples = inputs_np.shape[0]\n",
    "        split_idx = int(0.8 * num_samples)\n",
    "        train_inputs_np, eval_inputs_np = inputs_np[:split_idx], inputs_np[split_idx:]\n",
    "        train_targets_np, eval_targets_np = targets_np[:split_idx], targets_np[split_idx:]\n",
    "        \n",
    "        # Create DMatrix for train and eval sets\n",
    "        dtrain = xgboost.DMatrix(train_inputs_np, label=train_targets_np)\n",
    "        deval = xgboost.DMatrix(eval_inputs_np, label=eval_targets_np)\n",
    "        evals = [(dtrain, \"train\"), (deval, \"eval\")]\n",
    "        self.evals_result = {}\n",
    "        self.model = xgboost.train(self.params, \n",
    "                                   dtrain, \n",
    "                                   num_boost_round=100, \n",
    "                                   evals=evals, \n",
    "                                   evals_result=self.evals_result, \n",
    "                                   early_stopping_rounds=10\n",
    "                                   )\n",
    "        train_loss = self.evals_result[\"train\"][self.params[\"eval_metric\"]]\n",
    "        eval_loss = self.evals_result[\"eval\"][self.params[\"eval_metric\"]]\n",
    "\n",
    "        return train_loss, eval_loss\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Predict outputs for given inputs using the trained model.\n",
    "        \"\"\"\n",
    "        preds = XGBoostAutoGrad.apply(self.model, inputs)\n",
    "        nr, nb, dx = inputs.shape\n",
    "        _, dz = preds.shape\n",
    "        z_mu = preds.view(nr, nb, dz)[...,:int(dz/2)]\n",
    "        z_std = torch.abs(preds.view(nr, nb, dz)[..., int(dz/2):]) \n",
    "\n",
    "        return z_mu, z_std\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the trained model to the specified path.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet.\")\n",
    "        self.model.save_model(path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Load a trained model from the specified path.\n",
    "        \"\"\"\n",
    "        self.model = xgboost.Booster()\n",
    "        self.model.load_model(path)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass: call predict internally.\n",
    "        \"\"\"\n",
    "        return self.predict(inputs)\n",
    "\n",
    "class XGBoostAutoGrad(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(model, x):\n",
    "        \"\"\"\n",
    "        Predict outputs for given inputs using the trained model.\n",
    "        \"\"\"\n",
    "        nr, nb, dx = x.shape\n",
    "        x_2d = x.view(nr*nb, dx)\n",
    "        x_np = x_2d.clone().detach().cpu().numpy()\n",
    "        dmatrix = xgboost.DMatrix(x_np)\n",
    "        preds = model.predict(dmatrix)\n",
    "        preds_tensor = torch.tensor(preds, dtype=x.dtype, device=x.device)\n",
    " \n",
    "        return preds_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        model, x = inputs\n",
    "        preds = output\n",
    "        ctx.save_for_backward(x, preds)\n",
    "        ctx.model = model\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients w.r.t. the inputs using finite differences.\n",
    "        \"\"\"\n",
    "        x, preds = ctx.saved_tensors\n",
    "        nr, nb, dx = x.shape\n",
    "        x_2d = x.view(nr*nb, dx)\n",
    "        x_np = x_2d.clone().detach().cpu().numpy()\n",
    "        _, dz = preds.shape\n",
    "        preds_np = preds.clone().detach().cpu().numpy()\n",
    "\n",
    "        # Finite differences for gradient approximation\n",
    "        epsilon = 1e-2\n",
    "        grads = np.zeros((nr*nb, dz, dx))\n",
    "        for i in range(x_np.shape[1]):  # Loop over each feature\n",
    "            perturbed_inputs = x_np.copy()\n",
    "            perturbed_inputs[:, i] += epsilon  # Apply small perturbation to the i-th feature\n",
    "            perturbed_dmatrix = xgboost.DMatrix(perturbed_inputs)\n",
    "            perturbed_preds = ctx.model.predict(perturbed_dmatrix, output_margin=True)\n",
    "            \n",
    "            # Compute gradient approximation for the i-th feature\n",
    "            grads[..., i] = (perturbed_preds - preds_np) / epsilon\n",
    "\n",
    "        # Convert gradients to PyTorch tensors\n",
    "        grad_input = torch.tensor(grads, dtype=x.dtype, device=x.device)\n",
    "\n",
    "        return None, grad_input*grad_output.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "x = torch.rand(100, 1, 2, requires_grad=True)  # Enable gradients for input\n",
    "y = 2 * x[..., 0] - 3 * x[..., 1] + 0.5  # Linear relationship\n",
    "\n",
    "# Define XGBoost model wrapper\n",
    "xgb_model_args = {\"objective\": \"reg:squarederror\",\n",
    "                  \"max_depth\": 3,\n",
    "                  \"eta\": 0.1,\n",
    "                  \"eval_metric\": \"rmse\"\n",
    "                  }\n",
    "model = XGBoost(xgb_model_args)\n",
    "\n",
    "# Fit the model\n",
    "model.train(x, y)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(x)\n",
    "\n",
    "# Define a loss function (mean squared error)\n",
    "loss = torch.mean((predictions - y) ** 2)\n",
    "\n",
    "# Backpropagate to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradients of the inputs:\")\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 2]), torch.Size([100]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activephasemap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
